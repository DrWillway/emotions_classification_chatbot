{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQjWByH3ZBC8"
      },
      "source": [
        "# Text Generation using FNet\n",
        "\n",
        "**Description:** FNet transformer for text generation in Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kC_VaFWZBDB"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D8Seho5mZBDB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-14 04:24:00.583678: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-04-14 04:24:00.944951: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-14 04:24:00.945036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-14 04:24:01.003426: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-14 04:24:01.154422: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-04-14 04:24:01.156399: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-14 04:24:03.136346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Defining hyperparameters\n",
        "\n",
        "VOCAB_SIZE = 8192\n",
        "MAX_SAMPLES = 50000\n",
        "BUFFER_SIZE = 20000\n",
        "MAX_LENGTH = 100\n",
        "EMBED_DIM = 256\n",
        "LATENT_DIM = 512\n",
        "NUM_HEADS = 8\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sentimental' 'afraid' 'proud' 'faithful' 'terrified' 'joyful' 'angry'\n",
            " 'sad' 'jealous' 'grateful' 'prepared' 'embarrassed' 'excited' 'annoyed'\n",
            " 'lonely' 'ashamed' 'guilty' 'surprised' 'nostalgic' 'confident' 'furious'\n",
            " 'disappointed' 'caring' 'trusting' 'disgusted' 'anticipating' 'anxious'\n",
            " 'hopeful' 'content' 'impressed' 'apprehensive' 'devastated']\n"
          ]
        }
      ],
      "source": [
        "# Load train, valid, and test datasets from CSV files\n",
        "train_df = pd.read_csv('empathetic_dialogues/train.csv', on_bad_lines='skip')\n",
        "valid_df = pd.read_csv('empathetic_dialogues/valid.csv', on_bad_lines='skip')\n",
        "test_df = pd.read_csv('empathetic_dialogues/test.csv', on_bad_lines='skip')\n",
        "\n",
        "# Concatenate the datasets\n",
        "df = pd.concat([train_df, valid_df, test_df], ignore_index=True)\n",
        "\n",
        "# Selecting the first 50% of the dataframe\n",
        "half_len = len(df) // 2\n",
        "df_half = df.iloc[:half_len]\n",
        "\n",
        "# Print unique values of the 'context' column\n",
        "unique_contexts = df['context'].unique()\n",
        "print(unique_contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Define the mapping\n",
        "# mapping = {\n",
        "#     'afraid': 'fearful',\n",
        "#     'angry': 'angry',\n",
        "#     'annoyed': 'angry',\n",
        "#     'anticipating': 'neutral',\n",
        "#     'anxious': 'fearful',\n",
        "#     'apprehensive': 'neutral',\n",
        "#     'ashamed': 'sad',\n",
        "#     'caring': 'neutral',\n",
        "#     'confident': 'neutral',\n",
        "#     'content': 'joyful',\n",
        "#     'devastated': 'sad',\n",
        "#     'disappointed': 'sad',\n",
        "#     'disgusted': 'angry',\n",
        "#     'embarrassed': 'sad',\n",
        "#     'excited': 'joyful',\n",
        "#     'faithful': 'neutral',\n",
        "#     'furious': 'angry',\n",
        "#     'grateful': 'joyful',\n",
        "#     'guilty': 'sad',\n",
        "#     'hopeful': 'neutral',\n",
        "#     'impressed': 'surprised',\n",
        "#     'jealous': 'angry',\n",
        "#     'joyful': 'joyful',\n",
        "#     'lonely': 'sad',\n",
        "#     'nostalgic': 'sad',\n",
        "#     'prepared': 'neutral',\n",
        "#     'proud': 'joyful',\n",
        "#     'sad': 'sad',\n",
        "#     'sentimental': 'sad',\n",
        "#     'surprised': 'surprised',\n",
        "#     'terrified': 'fearful',\n",
        "#     'trusting': 'neutral'\n",
        "# }\n",
        "\n",
        "# # Apply the mapping to the 'context' column\n",
        "# df['context'] = df['context'].map(mapping)\n",
        "\n",
        "# df['context'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = []\n",
        "answers = []\n",
        "\n",
        "conversation = []\n",
        "conversation_context = None\n",
        "\n",
        "for entry in df.itertuples():\n",
        "    # Check if the context changes\n",
        "    if entry.context != conversation_context:\n",
        "        # If there's already a conversation, split it into questions and answers\n",
        "        if conversation:\n",
        "            # Ensure that the conversation has at least one question and one answer\n",
        "            if len(conversation) >= 2:\n",
        "                # Iterate over the conversation, starting from the second utterance\n",
        "                for idx in range(0, len(conversation)):\n",
        "                    # If the utterance_idx is odd, it's an answer; otherwise, it's a question\n",
        "                    if idx % 2 == 1:\n",
        "                        user_entry_context = f\"{conversation_context}: {conversation[idx - 1]}\"\n",
        "                        questions.append(user_entry_context)\n",
        "                        answers.append(conversation[idx])\n",
        "        # Start a new conversation\n",
        "        conversation = [entry.utterance]\n",
        "        conversation_context = entry.context\n",
        "    else:\n",
        "        conversation.append(entry.utterance)\n",
        "\n",
        "# Create a new DataFrame from the lists\n",
        "qa_df = pd.DataFrame({'question': questions, 'answer': answers})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sentimental: I remember going to see the firew...</td>\n",
              "      <td>Was this a friend you were in love with_comma_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sentimental: This was a best friend. I miss her.</td>\n",
              "      <td>Where has she gone?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sentimental: We no longer talk.</td>\n",
              "      <td>Oh was this something that happened because of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>afraid:  it feels like hitting to blank wall w...</td>\n",
              "      <td>Oh ya? I don't really see how</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>afraid: dont you feel so.. its a wonder</td>\n",
              "      <td>I do actually hit blank walls a lot of times b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>afraid:  i virtually thought so.. and i used t...</td>\n",
              "      <td>Wait what are sweatings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>proud: Hi how are you doing today</td>\n",
              "      <td>doing good.. how about you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>proud: Im good_comma_ trying to understand how...</td>\n",
              "      <td>it's quite strange that you didnt imagine it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>faithful: I have never cheated on my wife.</td>\n",
              "      <td>And thats something you should never do_comma_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>faithful: Yea it hasn't been easy but I am pro...</td>\n",
              "      <td>What do you mean it hasn't been easy? How clos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>terrified: Job interviews always make me sweat...</td>\n",
              "      <td>Don't be nervous. Just be prepared.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>terrified: I feel like getting prepared and th...</td>\n",
              "      <td>Yes but if you stay calm it will be ok.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>joyful: Hi_comma_ this year_comma_ I was the f...</td>\n",
              "      <td>Sounds great! So what's your major?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>joyful: It is computer science. I am very happ...</td>\n",
              "      <td>Well pleased. You should be having brains_comm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>angry: I lost my job last year and got really ...</td>\n",
              "      <td>I am sorry to hear that. Did it happen out of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>angry: Yes_comma_ it was a complete surprise.</td>\n",
              "      <td>I am sorry to hear that. I hope it turned out ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>sad: During christmas a few years ago_comma_ I...</td>\n",
              "      <td>Wow_comma_ that must be terrible_comma_ I cann...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>sad: Since that day christmas has not been a g...</td>\n",
              "      <td>Wow_comma_ I am sorry to hear that_comma_ I wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>jealous: My coworker is allowed to work remote...</td>\n",
              "      <td>I work remotely_comma_ I wish that you could d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>jealous: I do too_comma_ it is unfortnuate bec...</td>\n",
              "      <td>Sometimes in life_comma_ it is not about perfo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             question  \\\n",
              "0   sentimental: I remember going to see the firew...   \n",
              "1    sentimental: This was a best friend. I miss her.   \n",
              "2                     sentimental: We no longer talk.   \n",
              "3   afraid:  it feels like hitting to blank wall w...   \n",
              "4            afraid: dont you feel so.. its a wonder    \n",
              "5   afraid:  i virtually thought so.. and i used t...   \n",
              "6                   proud: Hi how are you doing today   \n",
              "7   proud: Im good_comma_ trying to understand how...   \n",
              "8          faithful: I have never cheated on my wife.   \n",
              "9   faithful: Yea it hasn't been easy but I am pro...   \n",
              "10  terrified: Job interviews always make me sweat...   \n",
              "11  terrified: I feel like getting prepared and th...   \n",
              "12  joyful: Hi_comma_ this year_comma_ I was the f...   \n",
              "13  joyful: It is computer science. I am very happ...   \n",
              "14  angry: I lost my job last year and got really ...   \n",
              "15      angry: Yes_comma_ it was a complete surprise.   \n",
              "16  sad: During christmas a few years ago_comma_ I...   \n",
              "17  sad: Since that day christmas has not been a g...   \n",
              "18  jealous: My coworker is allowed to work remote...   \n",
              "19  jealous: I do too_comma_ it is unfortnuate bec...   \n",
              "\n",
              "                                               answer  \n",
              "0   Was this a friend you were in love with_comma_...  \n",
              "1                                 Where has she gone?  \n",
              "2   Oh was this something that happened because of...  \n",
              "3                       Oh ya? I don't really see how  \n",
              "4   I do actually hit blank walls a lot of times b...  \n",
              "5                             Wait what are sweatings  \n",
              "6                          doing good.. how about you  \n",
              "7        it's quite strange that you didnt imagine it  \n",
              "8   And thats something you should never do_comma_...  \n",
              "9   What do you mean it hasn't been easy? How clos...  \n",
              "10                Don't be nervous. Just be prepared.  \n",
              "11            Yes but if you stay calm it will be ok.  \n",
              "12                Sounds great! So what's your major?  \n",
              "13  Well pleased. You should be having brains_comm...  \n",
              "14  I am sorry to hear that. Did it happen out of ...  \n",
              "15  I am sorry to hear that. I hope it turned out ...  \n",
              "16  Wow_comma_ that must be terrible_comma_ I cann...  \n",
              "17  Wow_comma_ I am sorry to hear that_comma_ I wi...  \n",
              "18  I work remotely_comma_ I wish that you could d...  \n",
              "19  Sometimes in life_comma_ it is not about perfo...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-14 04:24:06.276933: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-14 04:24:06.277682: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and validation sets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((questions, answers))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((questions, answers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMT262fdZBDD"
      },
      "source": [
        "## Loading data\n",
        "\n",
        "We will be using the Cornell Dialog Corpus. We will parse the movie conversations into\n",
        "questions and answers sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzx0ucb8ZBDE"
      },
      "source": [
        "### Preprocessing and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BhDNi6H3ZBDE"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(sentence):\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "    # Removing \"_comma_\" from sentences\n",
        "    sentence = tf.strings.regex_replace(sentence, \"_comma_\", \",\")\n",
        "    # Adding a space between the punctuation and the last word to allow better tokenization\n",
        "    sentence = tf.strings.regex_replace(sentence, r\"([?.!,])\", r\" \\1 \")\n",
        "    # Replacing multiple continuous spaces with a single space\n",
        "    sentence = tf.strings.regex_replace(sentence, r\"\\s\\s+\", \" \")\n",
        "    # Replacing non-English words with spaces\n",
        "    sentence = tf.strings.regex_replace(sentence, r\"[^a-z?.!,]+\", \" \")\n",
        "    sentence = tf.strings.strip(sentence)\n",
        "    sentence = tf.strings.join([\"[start]\", sentence, \"[end]\"], separator=\" \")\n",
        "    return sentence\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    VOCAB_SIZE,\n",
        "    standardize=preprocess_text,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "# We will adapt the vectorizer to both the questions and answers\n",
        "# This dataset is batched to parallelize and speed up the process\n",
        "vectorizer.adapt(tf.data.Dataset.from_tensor_slices((questions + answers)).batch(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHiBLk_HZBDF"
      },
      "source": [
        "### Tokenizing and padding sentences using `TextVectorization`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "miJLMR6vZBDF"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(inputs, outputs):\n",
        "    inputs, outputs = vectorizer(inputs), vectorizer(outputs)\n",
        "    # One extra padding token to the right to match the output shape\n",
        "    outputs = tf.pad(outputs, [[0, 1]])\n",
        "    return (\n",
        "        {\"encoder_inputs\": inputs, \"decoder_inputs\": outputs[:-1]},\n",
        "        {\"outputs\": outputs[1:]},\n",
        "    )\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = (\n",
        "    train_dataset.cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "val_dataset = val_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKEcii7YZBDF"
      },
      "source": [
        "## Creating the FNet Encoder\n",
        "\n",
        "The FNet paper proposes a replacement for the standard attention mechanism used by the\n",
        "Transformer architecture (Vaswani et al., 2017).\n",
        "\n",
        "![Architecture](https://i.imgur.com/rLg47qU.png)\n",
        "\n",
        "The outputs of the FFT layer are complex numbers. To avoid dealing with complex layers,\n",
        "only the real part (the magnitude) is extracted.\n",
        "\n",
        "The dense layers that follow the Fourier transformation act as convolutions applied on\n",
        "the frequency domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fn-0W2BJZBDG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FNetEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Casting the inputs to complex64\n",
        "        inp_complex = tf.cast(inputs, tf.complex64)\n",
        "        # Projecting the inputs to the frequency domain using FFT2D and\n",
        "        # extracting the real part of the output\n",
        "        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n",
        "        proj_input = self.layernorm_1(inputs + fft)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HDMGQWIZBDG"
      },
      "source": [
        "## Creating the Decoder\n",
        "\n",
        "The decoder architecture remains the same as the one proposed by (Vaswani et al., 2017)\n",
        "in the original transformer architecture, consisting of an embedding, positional\n",
        "encoding, two masked multi-head attention layers and finally the dense output layers.\n",
        "The architecture that follows is taken from\n",
        "[Deep Learning with Python, second edition, chapter 11](https://www.manning.com/books/deep-learning-with-python-second-edition)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DC0zkNdEZBDG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class FNetDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
        "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(encoder_inputs)\n",
        "    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n",
        "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n",
        "    encoded_seq_inputs = keras.Input(\n",
        "        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n",
        "    )\n",
        "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(decoder_inputs)\n",
        "    x = FNetDecoder(EMBED_DIM, LATENT_DIM, NUM_HEADS)(x, encoded_seq_inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    decoder_outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "    decoder = keras.Model(\n",
        "        [decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"outputs\"\n",
        "    )\n",
        "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "    fnet = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"fnet\")\n",
        "    return fnet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YW3ZaZpZBDH"
      },
      "source": [
        "## Creating and Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WoAy5APkZBDH"
      },
      "outputs": [],
      "source": [
        "fnet = create_model()\n",
        "fnet.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Callback to save the Keras model at some frequency.\n",
        "checkpoint_filepath = './checkpoints/checkpoint.model.keras'\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVSuydBlZBDH"
      },
      "source": [
        "Here, the `epochs` parameter is set to a single epoch, but in practice the model will take around\n",
        "**20-30 epochs** of training to start outputting comprehensible sentences. Although accuracy\n",
        "is not a good measure for this task, we will use it just to get a hint of the improvement\n",
        "of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
            "5261/5261 [==============================] - 2041s 387ms/step - loss: 4.7097 - accuracy: 0.1979 - val_loss: 4.5243 - val_accuracy: 0.2015\n",
            "Epoch 2/50\n",
            "5261/5261 [==============================] - 2009s 382ms/step - loss: 4.5329 - accuracy: 0.2040 - val_loss: 4.3613 - val_accuracy: 0.2112\n",
            "Epoch 3/50\n",
            "5261/5261 [==============================] - 1974s 375ms/step - loss: 4.4990 - accuracy: 0.2073 - val_loss: 4.4468 - val_accuracy: 0.2056\n",
            "Epoch 4/50\n",
            "5261/5261 [==============================] - 1971s 375ms/step - loss: 4.5406 - accuracy: 0.2026 - val_loss: 4.4394 - val_accuracy: 0.2073\n",
            "Epoch 5/50\n",
            "5261/5261 [==============================] - 1974s 375ms/step - loss: 4.5565 - accuracy: 0.2029 - val_loss: 4.4482 - val_accuracy: 0.2055\n",
            "Epoch 6/50\n",
            "5261/5261 [==============================] - 1960s 373ms/step - loss: 4.5392 - accuracy: 0.2032 - val_loss: 4.4341 - val_accuracy: 0.2041\n",
            "Epoch 7/50\n",
            "5261/5261 [==============================] - 2025s 385ms/step - loss: 4.5351 - accuracy: 0.2030 - val_loss: 4.4462 - val_accuracy: 0.2040\n",
            "Epoch 8/50\n",
            "5261/5261 [==============================] - 2015s 383ms/step - loss: 4.5502 - accuracy: 0.2025 - val_loss: 4.4827 - val_accuracy: 0.1990\n",
            "Epoch 9/50\n",
            "5261/5261 [==============================] - 2025s 385ms/step - loss: 4.5843 - accuracy: 0.1994 - val_loss: 4.5597 - val_accuracy: 0.1962\n",
            "Epoch 10/50\n",
            "5261/5261 [==============================] - 2021s 384ms/step - loss: 4.5889 - accuracy: 0.1996 - val_loss: 4.4768 - val_accuracy: 0.2029\n",
            "Epoch 11/50\n",
            "5261/5261 [==============================] - 2047s 389ms/step - loss: 4.5689 - accuracy: 0.2017 - val_loss: 4.5007 - val_accuracy: 0.2023\n",
            "Epoch 12/50\n",
            "5261/5261 [==============================] - 1981s 377ms/step - loss: 4.5393 - accuracy: 0.2026 - val_loss: 4.4545 - val_accuracy: 0.2030\n",
            "Epoch 13/50\n",
            "5261/5261 [==============================] - 1829s 348ms/step - loss: 4.5413 - accuracy: 0.2028 - val_loss: 4.4652 - val_accuracy: 0.2034\n",
            "Epoch 14/50\n",
            "5261/5261 [==============================] - 1647s 313ms/step - loss: 4.5663 - accuracy: 0.1999 - val_loss: 4.4870 - val_accuracy: 0.2011\n",
            "Epoch 15/50\n",
            "5261/5261 [==============================] - 1641s 312ms/step - loss: 4.5838 - accuracy: 0.1996 - val_loss: 4.4882 - val_accuracy: 0.2017\n",
            "Epoch 16/50\n",
            "5261/5261 [==============================] - 1644s 313ms/step - loss: 4.5794 - accuracy: 0.2001 - val_loss: 4.4871 - val_accuracy: 0.2015\n",
            "Epoch 17/50\n",
            "5261/5261 [==============================] - 1646s 313ms/step - loss: 4.5775 - accuracy: 0.2001 - val_loss: 4.4754 - val_accuracy: 0.2054\n",
            "Epoch 18/50\n",
            "3529/5261 [===================>..........] - ETA: 6:57 - loss: 4.5080 - accuracy: 0.2059"
          ]
        }
      ],
      "source": [
        "fnet.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset, callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the final model\n",
        "fnet.save(\"./models/chatbot_correct_emotions_2.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0g3yGJwZBDH"
      },
      "source": [
        "## Performing inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFeJBSjEZBDI"
      },
      "outputs": [],
      "source": [
        "VOCAB = vectorizer.get_vocabulary()\n",
        "\n",
        "\n",
        "def decode_sentence(input_sentence):\n",
        "    # Mapping the input sentence to tokens and adding start and end tokens\n",
        "    tokenized_input_sentence = vectorizer(\n",
        "        tf.constant(\"[start] \" + preprocess_text(input_sentence) + \" [end]\")\n",
        "    )\n",
        "    # Initializing the initial sentence consisting of only the start token.\n",
        "    tokenized_target_sentence = tf.expand_dims(VOCAB.index(\"[start]\"), 0)\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        # Get the predictions\n",
        "        predictions = fnet.predict(\n",
        "            {\n",
        "                \"encoder_inputs\": tf.expand_dims(tokenized_input_sentence, 0),\n",
        "                \"decoder_inputs\": tf.expand_dims(\n",
        "                    tf.pad(\n",
        "                        tokenized_target_sentence,\n",
        "                        [[0, MAX_LENGTH - tf.shape(tokenized_target_sentence)[0]]],\n",
        "                    ),\n",
        "                    0,\n",
        "                ),\n",
        "            }\n",
        "        )\n",
        "        # Calculating the token with maximum probability and getting the corresponding word\n",
        "        sampled_token_index = tf.argmax(predictions[0, i, :])\n",
        "        sampled_token = VOCAB[sampled_token_index.numpy()]\n",
        "        # If sampled token is the end token then stop generating and return the sentence\n",
        "        if tf.equal(sampled_token_index, VOCAB.index(\"[end]\")):\n",
        "            break\n",
        "        decoded_sentence += sampled_token + \" \"\n",
        "        tokenized_target_sentence = tf.concat(\n",
        "            [tokenized_target_sentence, [sampled_token_index]], 0\n",
        "        )\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decode_sentence(\"sadness: My sister got into a car accident.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decode_sentence(\"sadness: My dog died.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decode_sentence(\"fear: I am afraid to have a car crash.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation_fnet",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
